# ğŸ¡ Vaasthu Vision AI - AI-Powered Vaasthu Consultant


## ğŸš€ Project Overview  

Vaasthu Vision AI is an intelligent GenAI system designed to provide authentic guidance, directional insights, and remedial suggestions based on Vaasthu Shastra. Unlike generic chatbots, it uses a Retrieval-Augmented Generation (RAG) architecture to ensure that every response is accurate, reliable, and context-aware. 

The system carefully routes queries using similarity scores and critical keyword checks, preventing wrong or hallucinated answers while always prioritizing trustworthy knowledge from its Vaasthu knowledge base.

## ğŸ¥ Project Demo  

Iâ€™ve shared my project demo and explanation on LinkedIn â€” check them out below:  

- ğŸ¬ [Demo Video (LinkedIn)](https://www.linkedin.com/posts/shiva-prasad-naroju-4772a6184_genai-artificialintelligence-machinelearning-activity-7353397783669731329-wMpR?utm_source=share&utm_medium=member_desktop&rcm=ACoAACt4LRsBQ69c1XBIukR81m4Iy7dej5736CU)

- ğŸ“– [Project Explanation (LinkedIn Post)](https://www.linkedin.com/feed/update/urn:li:activity:7353392679910559744?utm_source=share&utm_medium=member_desktop&rcm=ACoAACt4LRsBQ69c1XBIukR81m4Iy7dej5736CU)

ğŸ‘‰ Click to watch the full explanation or directly view the video output.  


## ğŸ¯ Objective  
To build an AI assistant that:
- Understands Vaasthu rules deeply
- Provides reliable answers
- Avoids hallucinations
- Runs fast on web (connected to a slick frontend)

## ğŸ› ï¸ Tech Stack  
- **Frontend**: Built the website using bolt ai and customized as desired. 
- **Backend**: Python (FastAPI / Streamlit for local)  
- **LLM**: LLaMA3-8B-8192 via **Groq API**  
- **Vector DB**: **Qdrant** with `all-MiniLM-L6-v2` embeddings  
- **RAG**: LangChain-powered pipeline  

## ğŸ§© Data & Design Decisions  
### ğŸ”§ Data Transformation
- Started with 40 structured Vaasthu elements in JSON  
- Converted to 350+ high-quality natural-language rules  
- Added metadata: `zone`, `rule_id`, `category`  
- Stored using `RULE_START` and `RULE_END` delimiters  


### ğŸ§  Prompt Engineering  
- Final prompt: minimalistic, 4 - 6 lines of answer.
- Designed for clarity, consistency, and production use  
- Used temperature=0 and top_p=1 for deterministic output


### âš™ï¸ RAG Flow
1. Query â†’ Embedding  
2. Qdrant â†’ Top 3 relevant rules  
3. Custom prompt â†’ Groq LLM (LLaMA3)  
4. Final response â†’ Displayed in UI

## System Architecture:

```
graph TD
    A[ğŸ“ User Question] --> B[ğŸ” Critical Keyword Check]
    B -->|âœ… Keyword Match| C[âš¡ RAG QA Chain]
    B -->|âŒ No Keyword| D[ğŸ” Vectorstore Retrieval]
    D --> E[ğŸ“Š Qdrant Similarity Score]
    E --> F{ğŸ“‹ Confidence Thresholds}
    F -->|High| C
    F -->|Medium| G[âŒ "I don't know" Response]
    F -->|Low| H[ğŸ’¬ Fallback Chat Chain]
    C --> I[âœ¨ Final Vaasthu Answer]
    G --> I
    H --> I
```

### Query Router  

This project implements a **smart query routing system** that decides whether a userâ€™s question should be answered via **RAG pipeline** (vector database retrieval) or by an **LLM fallback**, based on **similarity scores** and **critical keywords**.  

### âš¡ Workflow Overview  

1. **User enters a query**  
2. The system runs a **similarity search** on the vector database.  
3. A **similarity score** is calculated for the top retrieved chunks.  
4. Based on this score and rules, the query is routed:  

### ğŸ” Query Routing Logic  

- **Case 1: High Confidence (â‰¥ HIGH_THRESHOLD)**  
   - âœ… Strong match found in vector DB  
   - ğŸ‘‰ Response generated by **RAG pipeline (qa_chain)**  

- **Case 2: Low Confidence (< LOW_THRESHOLD)**  
   - âš ï¸ Retrieved chunks are unreliable  
   - ğŸ‘‰ Routed to **LLM fallback**, which replies:  
   - `"Sorry, I donâ€™t have an idea about this query."`  

- **Case 3: Critical Keywords Override**  
   - ğŸ”‘ Even if similarity score is **below LOW_THRESHOLD**,  
   - If the query contains **critical keywords** (e.g., *Kitchen, Bathroom, Hall*),  
   - ğŸ‘‰ Still answered through **RAG pipeline** (domain relevance guaranteed)  

- **Case 4: Nonsense / Out-of-Domain Queries**  
   - ğŸš« No relevant match + no critical keywords  
   - ğŸ‘‰ Routed to **LLM fallback** for casual/nonsense handling
  

## ğŸ¯ Key Features  

- **Confidence-based Routing** â†’ prevents misleading answers from weak retrievals.  
- **Domain Awareness** â†’ critical keywords always prioritize vector DB results.  
- **LLM Fallback** â†’ handles nonsense or completely unrelated queries.  
- **Accuracy & Safety First** â†’ ensures only reliable information is returned.
- **Contribute Feature** â†’ Allows users to submit data or upload files for review. After admin verification, contributions can be incorporated into the project.



## ğŸ§  Learnings  
- Prompt structure impacts hallucination significantly  
- Simpler is better: hardcoded rules > complex intent classifier  
- Fast, reliable LLMs like Groq drastically improve UX  
- Semantic granularity in rules increases RAG accuracy


## ğŸŒ Website of frontend only without backend integration:
ğŸ”— Click here : [Visit Site](https://jazzy-entremet-70cc2a.netlify.app/)

or

website : https://jazzy-entremet-70cc2a.netlify.app/

## ğŸ’¼ Ready for Production  
The system is ready for real-world integration and can be expanded to:
- Multiple languages  
- Room-by-room suggestions  
- Vaastu-based house plan checker

## ğŸ³ Docker Setup

We provide a Docker image for **Vaasthu Vision AI** so you can run the app anywhere without installing dependencies.

### **Prerequisites**
- Docker Desktop installed: [https://www.docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop)

- A `.env` file with:

  ```env

  - QDRANT_URL=<your-qdrant-cloud-url>
  
  - QDRANT_API_KEY=<your-qdrant-api-key>
  
  - GROQ_API_KEY=<your-groq-api-key>

### Run via Docker Hub Image:

Pull the latest image from Docker Hub:

- docker pull docker.io/shivaprasadnaroju/vaasthu-vision-ai:latest

Run the container with your .env:

- docker run -p 8000:8000 --env-file .env docker.io/shivaprasadnaroju/vaasthu-vision-ai:latest

Visit: http://localhost:8000/docs to access FastAPI Swagger UI.

### Notes:

- The image is preconfigured to connect to Qdrant Cloud via .env.

- For a local Qdrant setup, a separate Docker Compose file can be used

## ğŸ™Œ Special Thanks  
Inspired by traditional Indian architecture wisdom and empowered by modern AI.


## ğŸ“¬ Connect with Me  
Iâ€™m open to collaborations, feedback, or AI-based consulting.  
ğŸ“§ Email: shivanaroju26@gmail.com


