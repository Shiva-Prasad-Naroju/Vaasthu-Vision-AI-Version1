# 🏡 Vaasthu Vision AI - AI-Powered Vaasthu Consultant


## 🚀 Project Overview  

Vaasthu Vision AI is an intelligent GenAI system designed to provide authentic guidance, directional insights, and remedial suggestions based on Vaasthu Shastra. Unlike generic chatbots, it uses a Retrieval-Augmented Generation (RAG) architecture to ensure that every response is accurate, reliable, and context-aware. 

The system carefully routes queries using similarity scores and critical keyword checks, preventing wrong or hallucinated answers while always prioritizing trustworthy knowledge from its Vaasthu knowledge base.

## 🎥 Project Demo  

I’ve shared my project demo and explanation on LinkedIn — check them out below:  

- 🎬 [Demo Video (LinkedIn)](https://www.linkedin.com/posts/shiva-prasad-naroju-4772a6184_genai-artificialintelligence-machinelearning-activity-7353397783669731329-wMpR?utm_source=share&utm_medium=member_desktop&rcm=ACoAACt4LRsBQ69c1XBIukR81m4Iy7dej5736CU)

- 📖 [Project Explanation (LinkedIn Post)](https://www.linkedin.com/feed/update/urn:li:activity:7353392679910559744?utm_source=share&utm_medium=member_desktop&rcm=ACoAACt4LRsBQ69c1XBIukR81m4Iy7dej5736CU)

👉 Click to watch the full explanation or directly view the video output.  


## 🎯 Objective  
To build an AI assistant that:
- Understands Vaasthu rules deeply
- Provides reliable answers
- Avoids hallucinations
- Runs fast on web (connected to a slick frontend)

## 🛠️ Tech Stack  
- **Frontend**: Built the website using bolt ai and customized as desired. 
- **Backend**: Python (FastAPI / Streamlit for local)  
- **LLM**: LLaMA3-8B-8192 via **Groq API**  
- **Vector DB**: **Qdrant** with `all-MiniLM-L6-v2` embeddings  
- **RAG**: LangChain-powered pipeline  

## 🧩 Data & Design Decisions  
### 🔧 Data Transformation
- Started with 40 structured Vaasthu elements in JSON  
- Converted to 350+ high-quality natural-language rules  
- Added metadata: `zone`, `rule_id`, `category`  
- Stored using `RULE_START` and `RULE_END` delimiters  


### 🧠 Prompt Engineering  
- Final prompt: minimalistic, 4 - 6 lines of answer.
- Designed for clarity, consistency, and production use  
- Used temperature=0 and top_p=1 for deterministic output


### ⚙️ RAG Flow
1. Query → Embedding  
2. Qdrant → Top 3 relevant rules  
3. Custom prompt → Groq LLM (LLaMA3)  
4. Final response → Displayed in UI

## System Architecture:

```
graph TD
    A[📝 User Question] --> B[🔍 Critical Keyword Check]
    B -->|✅ Keyword Match| C[⚡ RAG QA Chain]
    B -->|❌ No Keyword| D[🔎 Vectorstore Retrieval]
    D --> E[📊 Qdrant Similarity Score]
    E --> F{📋 Confidence Thresholds}
    F -->|High| C
    F -->|Medium| G[❌ "I don't know" Response]
    F -->|Low| H[💬 Fallback Chat Chain]
    C --> I[✨ Final Vaasthu Answer]
    G --> I
    H --> I
```

### Query Router  

This project implements a **smart query routing system** that decides whether a user’s question should be answered via **RAG pipeline** (vector database retrieval) or by an **LLM fallback**, based on **similarity scores** and **critical keywords**.  

### ⚡ Workflow Overview  

1. **User enters a query**  
2. The system runs a **similarity search** on the vector database.  
3. A **similarity score** is calculated for the top retrieved chunks.  
4. Based on this score and rules, the query is routed:  

### 🔎 Query Routing Logic  

- **Case 1: High Confidence (≥ HIGH_THRESHOLD)**  
   - ✅ Strong match found in vector DB  
   - 👉 Response generated by **RAG pipeline (qa_chain)**  

- **Case 2: Low Confidence (< LOW_THRESHOLD)**  
   - ⚠️ Retrieved chunks are unreliable  
   - 👉 Routed to **LLM fallback**, which replies:  
   - `"Sorry, I don’t have an idea about this query."`  

- **Case 3: Critical Keywords Override**  
   - 🔑 Even if similarity score is **below LOW_THRESHOLD**,  
   - If the query contains **critical keywords** (e.g., *Kitchen, Bathroom, Hall*),  
   - 👉 Still answered through **RAG pipeline** (domain relevance guaranteed)  

- **Case 4: Nonsense / Out-of-Domain Queries**  
   - 🚫 No relevant match + no critical keywords  
   - 👉 Routed to **LLM fallback** for casual/nonsense handling
  

## 🎯 Key Features  

- **Confidence-based Routing** → prevents misleading answers from weak retrievals.  
- **Domain Awareness** → critical keywords always prioritize vector DB results.  
- **LLM Fallback** → handles nonsense or completely unrelated queries.  
- **Accuracy & Safety First** → ensures only reliable information is returned.
- **Contribute Feature** → Allows users to submit data or upload files for review. After admin verification, contributions can be incorporated into the project.



## 🧠 Learnings  
- Prompt structure impacts hallucination significantly  
- Simpler is better: hardcoded rules > complex intent classifier  
- Fast, reliable LLMs like Groq drastically improve UX  
- Semantic granularity in rules increases RAG accuracy


## 🌐 Website of frontend only without backend integration:
🔗 Click here : [Visit Site](https://jazzy-entremet-70cc2a.netlify.app/)

or

website : https://jazzy-entremet-70cc2a.netlify.app/

## 💼 Ready for Production  
The system is ready for real-world integration and can be expanded to:
- Multiple languages  
- Room-by-room suggestions  
- Vaastu-based house plan checker

## 🐳 Docker Setup

We provide a Docker image for **Vaasthu Vision AI** so you can run the app anywhere without installing dependencies.

### **Prerequisites**
- Docker Desktop installed: [https://www.docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop)

- A `.env` file with:

  ```env

  - QDRANT_URL=<your-qdrant-cloud-url>
  
  - QDRANT_API_KEY=<your-qdrant-api-key>
  
  - GROQ_API_KEY=<your-groq-api-key>

### Run via Docker Hub Image:

Pull the latest image from Docker Hub:

- docker pull docker.io/shivaprasadnaroju/vaasthu-vision-ai:latest

Run the container with your .env:

- docker run -p 8000:8000 --env-file .env docker.io/shivaprasadnaroju/vaasthu-vision-ai:latest

Visit: http://localhost:8000/docs to access FastAPI Swagger UI.

### Notes:

- The image is preconfigured to connect to Qdrant Cloud via .env.

- For a local Qdrant setup, a separate Docker Compose file can be used

## 🙌 Special Thanks  
Inspired by traditional Indian architecture wisdom and empowered by modern AI.


## 📬 Connect with Me  
I’m open to collaborations, feedback, or AI-based consulting.  
📧 Email: shivanaroju26@gmail.com


